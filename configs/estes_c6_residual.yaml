# estes_c6_residual.yaml
#
# RESIDUAL RL: RL agent learns small corrections on top of PID controller
#
# Key insight: PID already produces smooth control. Instead of fighting
# RL's tendency toward bang-bang, we let PID handle the base control
# and RL learns small adjustments for edge cases.
#
# final_action = PID_output + clip(RL_output * max_residual, -max_residual, max_residual)
#
# Benefits:
#   - Inherits smooth behavior from PID baseline
#   - RL only learns small corrections (easier problem)
#   - Robust: even if RL fails, PID provides reasonable control
#   - Faster training: starts with good baseline behavior
#
# Motor: C-class, 10.0 N·s total impulse
#        5.4N average thrust, 1.85s burn time
#
# Rocket: 98g dry mass, TWR=5.00
#
physics:
  dry_mass: 0.0978
  propellant_mass: 0.0123
  diameter: 0.0198
  length: 0.4
  num_fins: 4
  fin_span: 0.04
  fin_root_chord: 0.05
  fin_tip_chord: 0.025
  max_tab_deflection: 3.6
  tab_chord_fraction: 0.25
  tab_span_fraction: 0.5
  cd_body: 0.5
  cd_fins: 0.01
  cl_alpha: 2.0
  control_effectiveness: 1.0
  disturbance_scale: 0.0001
  damping_scale: 1.5
  initial_spin_std: 9.0
  max_roll_rate: 450.0
  # === RESIDUAL PID SETTINGS ===
  # RL learns corrections on top of PID
  use_residual_pid: true
  # Maximum RL correction: 0.3 = RL can adjust PID output by ±30%
  max_residual: 0.3
  # PID gains (tuned for this rocket)
  pid_Kp: 0.02   # Proportional gain
  pid_Ki: 0.005  # Integral gain
  pid_Kd: 0.05   # Derivative gain
  # Disable other action wrappers (residual PID handles everything)
  use_delta_actions: false
  action_smoothing_alpha: 0.0
  action_rate_limit: 0.0
  include_previous_action: false
motor:
  name: estes_c6
  manufacturer: Estes
  designation: C6
  thrust_multiplier: 1.0
  diameter_mm: 18.0
  length_mm: 70.0
  total_mass_g: 24.0
  propellant_mass_g: 12.3
  case_mass_g: 11.7
  impulse_class: C
  total_impulse_Ns: 10.0
  avg_thrust_N: 5.4
  max_thrust_N: 14.0
  burn_time_s: 1.85
  thrust_curve:
    time_s:
    - 0.0
    - 0.04
    - 0.13
    - 0.5
    - 1.0
    - 1.5
    - 1.85
    thrust_N:
    - 0.0
    - 14.0
    - 12.0
    - 6.0
    - 5.0
    - 4.5
    - 0.0
environment:
  dt: 0.01
  max_episode_steps: 500
  initial_spin_rate_range:
  - -18.0
  - 18.0
  initial_tilt_range:
  - -5.0
  - 5.0
  enable_wind: true
  max_wind_speed: 5.0
  max_gust_speed: 2.5
  wind_variability: 0.3
  max_tilt_angle: 45.0
  min_altitude: -1.0
  max_altitude: 200
  normalize_observations: true
  obs_clip_value: 10.0
reward:
  # Simpler reward: focus on spin reduction, let PID handle smoothness
  altitude_reward_scale: 0.01
  # Stronger spin penalty - we want RL to help PID reduce spin faster
  spin_penalty_scale: -0.01
  low_spin_bonus: 3.0
  low_spin_threshold: 5.0
  # Zero-spin bonus for getting very close to zero
  zero_spin_bonus: 5.0
  zero_spin_threshold: 2.0
  # Light control penalties (PID is already smooth)
  control_effort_penalty: -0.005
  control_smoothness_penalty: -0.05
  spin_oscillation_penalty: -0.01
  # Reduced sign reversal penalty (PID handles this)
  sign_reversal_penalty: -0.2
  saturation_penalty: -0.5
  # No elastic net needed (PID + small residual is inherently smooth)
  action_l1_penalty: 0.0
  action_l2_penalty: 0.0
  # Settling incentives
  early_settling_bonus: 100.0
  settling_spin_threshold: 3.0  # Tighter threshold with PID help
  settling_time_limit: 0.5
  settling_deadline_penalty: -50.0
  early_phase_spin_multiplier: 2.0
  success_bonus: 100.0
  crash_penalty: -50.0
  use_potential_shaping: false
  gamma: 0.99
ppo:
  # Higher learning rate - simpler problem
  learning_rate: 0.0003
  n_steps: 2048
  batch_size: 64
  n_epochs: 10
  gamma: 0.99
  gae_lambda: 0.95
  clip_range: 0.2
  clip_range_vf: null
  # Moderate entropy
  ent_coef: 0.01
  vf_coef: 0.5
  max_grad_norm: 0.5
  normalize_advantage: true
  # Smaller network - simpler problem (just learning residuals)
  policy_net_arch:
  - 128
  - 128
  value_net_arch:
  - 128
  - 128
  activation: tanh
  # Shorter training - should converge faster with PID baseline
  total_timesteps: 500000
  n_envs: 8
  device: auto
curriculum:
  enabled: false
  stages: []
  episodes_to_evaluate: 100
  advancement_threshold: 0.8
logging:
  log_dir: logs
  save_dir: models
  tensorboard_log: true
  save_freq: 10000
  keep_checkpoints: 5
  eval_freq: 5000
  n_eval_episodes: 20
  log_episode_freq: 10
  experiment_name: rocket_estes_c6_residual
  tags:
  - estes_c6
  - spin_control
  - residual_pid
  - hybrid_control
